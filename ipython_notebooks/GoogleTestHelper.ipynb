{
 "metadata": {
  "name": "",
  "signature": "sha256:979dc32d90a459885ca3358c39c643e372c526dd4921d6fe37c2875d20cc0461"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import operator\n",
      "import sys\n",
      "\n",
      "from lxml       import etree\n",
      "from optparse   import OptionParser\n",
      "\n",
      "from copy \t\timport deepcopy\n",
      "from gensim \t\timport \tutils, matutils\n",
      "from gensim.models \timport word2vec\n",
      "\n",
      "from nltk.corpus \timport wordnet as wn\n",
      "from nltk.tag \t\timport pos_tag\n",
      "from nltk.tokenize \timport word_tokenize\n",
      "from nltk.stem      \timport WordNetLemmatizer\n",
      "\n",
      "from numpy \t\timport dot, multiply, add\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "final_model = word2vec.Word2Vec.load_word2vec_format('/Users/bansal/Desktop/NLP/Project/English-Lexicalized-Text-Substituion/GoogleNews-vectors-negative300.bin', binary=1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# List of tags, only which we consider are important. All other words dont add much to the context.\n",
      "important_tags = ['NN', 'NNS', 'VB', 'VBP', 'VBN', 'VBG', 'VBD', 'VBZ', 'NNP', 'JJ', 'JJR', 'JJS', 'RB', 'N', 'PRP$', 'PRP']\n",
      "\n",
      "###############################################################################\n",
      "# Helper functions.\n",
      "\n",
      "def get_wordnet_pos(treebank_tag):\n",
      "    \"\"\"\n",
      "    This function would return corresponding wordnet POS tag for \n",
      "    penn-tree bank POS tag.\n",
      "\n",
      "    \"\"\"\n",
      "    if treebank_tag.startswith('J'):\n",
      "        return wn.ADJ\n",
      "    elif treebank_tag.startswith('V'):\n",
      "        return wn.VERB\n",
      "    elif treebank_tag.startswith('N'):\n",
      "        return wn.NOUN\n",
      "    elif treebank_tag.startswith('R'):\n",
      "        return wn.ADV\n",
      "    else:\n",
      "        return None\n",
      "    \n",
      "def get_imp_words(tagged_sentence):\n",
      "    \"\"\"\n",
      "    This function would filter words in the sentence whose\n",
      "    tags don't belong to important tags list.\n",
      "\n",
      "    \"\"\"\n",
      "    tokens = filter(lambda x: x != None, map(lambda x: x if x[1] in important_tags else None, tagged_sentence))\n",
      "    #print tokens\n",
      "    return tokens\n",
      "\n",
      "def cosine(ww1, ww2):\n",
      "    return dot(matutils.unitvec(ww1), matutils.unitvec(ww2))\n",
      "\n",
      "###############################################################################\n",
      "# Replacement Helpers.\n",
      "\n",
      "def find_replacements_helper(imp_words, word, index, lwindow, rwindow, add):\n",
      "    \"\"\"\n",
      "    This function actually runs the model and find replacements for the word.\n",
      "\n",
      "    \"\"\"\n",
      "    #print word + \" \" + imp_words + \" \" + add\n",
      "\n",
      "    left = index - lwindow if index - lwindow >= 0 else 0\n",
      "    right = index + rwindow if index + rwindow <= len(imp_words) else len(imp_words)\n",
      "    \n",
      "    context_words = imp_words[left:index] + imp_words[index + 1:right]\n",
      "    #print context_words, word\n",
      "    # Gather all the context words in one vector.\n",
      "\n",
      "    base_unison = None\n",
      "    for x in context_words:\n",
      "        try:\n",
      "            if base_unison is None:\n",
      "                base_unison = deepcopy(final_model.syn0[final_model.vocab[x].index])\n",
      "            else:\n",
      "                if add:\n",
      "                    base_unison += final_model.syn0[final_model.vocab[x].index]\n",
      "                else:\n",
      "                    base_unison = multiply(base_unison, final_model.syn0[final_model.vocab[x].index])\n",
      "        except KeyError, ex:\n",
      "            print \"Warning: \" + x + \" is not in entire corpus\" \n",
      "            pass\n",
      "    \n",
      "    # Create a vector having context words and word to replace.\n",
      "    if add:\n",
      "        context_word_vector = base_unison + final_model.syn0[final_model.vocab[word].index]\n",
      "    else:\n",
      "        context_word_vector = multiply(base_unison, final_model.syn0[final_model.vocab[word].index])\n",
      "   \n",
      "    results = {}\n",
      "    for replacement, xx in final_model.most_similar(word,  topn=30):\n",
      "            # Ignore itself as a replacement.\n",
      "            if replacement in context_words:\n",
      "                continue\n",
      "            # Get rid of cases like \"fix\" and \"fixing\".\n",
      "            if word in replacement or replacement in word:\n",
      "                continue\n",
      "            # Replace only with the same POS tag.\n",
      "            if replacement[-1] != word[-1]:\n",
      "                continue\n",
      "            \n",
      "            replacement_vector = final_model.syn0[final_model.vocab[replacement].index]\n",
      "            \n",
      "            if add:\n",
      "                context_repl_vector = base_unison + replacement_vector\n",
      "            else:\n",
      "                context_repl_vector = multiply(base_unison, replacement_vector)\n",
      "\n",
      "            results[replacement] = cosine(context_word_vector, context_repl_vector) \n",
      "            #print context_word_vector +\" \" + context_repl_vector\n",
      "    #print results        \n",
      "    return (word, map(lambda x: x[0], sorted(results.iteritems(), key=operator.itemgetter(1), reverse=True)[:10]))\n",
      "\n",
      "###############################################################################\n",
      "def find_replacements(sentence, lwindow, rwindow, add=False):\n",
      "    \"\"\"\n",
      "    This function would be used to find replacements for the word present\n",
      "    inside the sentence.\n",
      "\n",
      "    @sentence: Actual sentence in which word is present.\n",
      "    @lwindow : Number of context words in the left of the replacement.\n",
      "    @rwindow : Number of context words in the right of the replacement.\n",
      "    @add     : Whether we are going to add the vectors. \n",
      "               Otherwise default to multiply.\n",
      "\n",
      "    \"\"\"\n",
      "    # Remove the START and END temporarily and tag the data.\n",
      "    word       = sentence[sentence.index('_START_') + 7 : sentence.index('_END_')]\n",
      "    word_index = nltk.word_tokenize(sentence).index(\"_START_\" + word + \"_END_\")\n",
      "    t_sentence = sentence[:sentence.index('_START_')] + word + sentence[sentence.index('_END_') + 5:]\n",
      "\n",
      "    # Tag the sentence and then bring the START and END back.\n",
      "    tagged_sentence = nltk.pos_tag(nltk.word_tokenize(t_sentence))\n",
      "    #print sentence, tagged_sentence\n",
      "\n",
      "    wnl = WordNetLemmatizer()\n",
      "    word_postag = get_wordnet_pos(tagged_sentence[word_index][1])\n",
      "    if word_postag:\n",
      "        word = wnl.lemmatize(word, pos=word_postag)\n",
      "    tagged_sentence[word_index] = [\"_START_\" + word + \"_END_\", tagged_sentence[word_index][1]]\n",
      "    \n",
      "    # Remove all the words, whose tags are not important and also\n",
      "    # get rid of smaller words.\n",
      "    imp_words = filter(lambda x: len(x[0]) > 2, get_imp_words(tagged_sentence))\n",
      "    #print imp_words\n",
      "\n",
      "    final_list = []\n",
      "    for i, x in enumerate(imp_words):\n",
      "        if x[0].startswith(\"_START_\"):\n",
      "            index = i\n",
      "            x[0] = x[0][7:x[0].index(\"_END_\")]\n",
      "            final_list.append(\"_START_\" + x[0].lower() + \"_\" + x[1][0].lower() + \"_END_\")\n",
      "            word = word.lower() #+ \"_\" + x[1][0].lower()\n",
      "            #print word\n",
      "        else:\n",
      "            # Lemmatize all the words.\n",
      "            word_postag = get_wordnet_pos(x[1])\n",
      "            temp = x[0]\n",
      "            if word_postag:\n",
      "                temp = wnl.lemmatize(x[0], pos=word_postag)\n",
      "            final_list.append(temp.lower()) # + \"_\" + x[1][0].lower())\n",
      "\n",
      "    try:\n",
      "        return find_replacements_helper(final_list, word, index, int(lwindow), int(rwindow) + 1, add)\n",
      "    except Exception:\n",
      "        return \"NONE\"\n",
      "\n",
      "###############################################################################\n",
      "\n",
      "def get_options():\n",
      "    parser = OptionParser()\n",
      "\n",
      "    # Options related to the model.\n",
      "    parser.add_option(\"--bin_file\", dest=\"bin_file\",\n",
      "                      help=\"Location from where bin file is to be read.\")\n",
      "\n",
      "    # Options related to testing.\n",
      "    parser.add_option(\"-x\", \"--xml_input\", dest=\"xml_input\",\n",
      "                      help=\"Xml input file provided by the English Lexical\\\n",
      "                            Substituition task.\")\n",
      "    parser.add_option(\"--add\", action=\"store_true\", dest=\"add\",\n",
      "                      help=\"If we want to add vectors in context for finding replacements.\")\n",
      "    parser.add_option(\"--lwindow\", dest=\"lwindow\", default=2,\n",
      "                      help=\"Number of words to the left of the words to be replaced.\")\n",
      "    parser.add_option(\"--rwindow\", dest=\"rwindow\", default=2,\n",
      "                      help=\"Number of words to the right of the words to be replaced.\")\n",
      "    parser.add_option(\"--output_to\", dest=\"output_file\",\n",
      "                      help=\"File name where output has to be written.\")\n",
      "    \n",
      "    opts, args = parser.parse_args()\n",
      "    \n",
      "    if not opts.bin_file:\n",
      "        sys.exit(\"Please give either bin file.\")\n",
      "\n",
      "    return (opts, args)\n",
      "\n",
      "###############################################################################\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # Either train or load the model.\n",
      "\n",
      "    #final_model = word2vec.Word2Vec.load_word2vec_format(opts.bin_file, binary=1)\n",
      "    #    final_model = io_utils.load(opts.pkl_file, MySpace)\n",
      "\n",
      "    # Load and test the XMl input file.\n",
      "    #if not opts.xml_input:\n",
      "    #    sys.exit(\"\")\n",
      "\n",
      "    # Read the file In.\n",
      "    sxml = \"\"\n",
      "    with open('/Users/bansal/Desktop/NLP/Project/English-Lexicalized-Text-Substituion/TaskTestData/test/lexsub_test_cleaned.xml') as xml_file:\n",
      "        sxml = xml_file.read()\n",
      "\n",
      "    parser = etree.XMLParser(recover=True)\n",
      "    tree   = etree.fromstring(sxml, parser=parser)\n",
      "\n",
      "    f = open('test_file', \"w\")\n",
      "    for el in tree.findall('lexelt'):\n",
      "        for ch in el.getchildren():\n",
      "            for ch1 in ch.getchildren():\n",
      "                sentence = ch1.text\n",
      "                \n",
      "                #word     = sentence[sentence.index('_START_') + 7 : sentence.index('_END_')]\n",
      "                #index    = nltk.word_tokenize(sentence).index(\"_START_\" + word + \"_END_\")\n",
      "                #sentence = sentence[:sentence.index('_START_')] + word + sentence[sentence.index('_END_') + 5:]\n",
      "                \n",
      "                #print sentence, word, index\n",
      "                result = find_replacements(sentence, 2, 2, False)\n",
      "                values = \";\".join(result[1])\n",
      "                print result[0] + \" \" + str(ch.items()[0][1]) + \" :: \" + values\n",
      "                f.write(str(result[0].replace(\"_\", \".\")) + \" \" + str(ch.items()[0][1]) + \" :: \" + values)\n",
      "                f.write(\"\\n\")\n",
      "    f.close()\n",
      "    print \"Output file written.\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "side 301 :: Domaine_Rouse;bobbly_surface;opposite;Nick_Walne;Big_disbalance\n",
        "side 302 :: Nick_Walne;Big_disbalance;Domaine_Rouse;opposite;bobbly_surface"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 303 :: Nick_Walne;Big_disbalance;Domaine_Rouse;bobbly_surface;opposite"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 304 :: Domaine_Rouse;opposite;Nick_Walne;Big_disbalance;bobbly_surface"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 305 :: opposite;Domaine_Rouse;Nick_Walne;Big_disbalance;bobbly_surface"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 306 :: Big_disbalance;opposite;Domaine_Rouse;bobbly_surface;Nick_Walne"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 307 :: Domaine_Rouse;Big_disbalance;Nick_Walne;opposite;bobbly_surface"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 308 :: Big_disbalance;Domaine_Rouse;opposite;bobbly_surface;Nick_Walne"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 309 :: Big_disbalance;Domaine_Rouse;Nick_Walne;opposite;bobbly_surface"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "side 310 :: Big_disbalance;Domaine_Rouse;opposite;Nick_Walne;bobbly_surface"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 311 :: Tell;Simpson_ghostwritten_hypothetical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 312 :: Simpson_ghostwritten_hypothetical;Tell"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 313 :: Tell;Simpson_ghostwritten_hypothetical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 314 :: Tell;Simpson_ghostwritten_hypothetical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 315 :: Simpson_ghostwritten_hypothetical;Tell"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 316 :: Tell;Simpson_ghostwritten_hypothetical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 317 :: Tell;Simpson_ghostwritten_hypothetical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning: reep is not in entire corpus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 318 :: Simpson_ghostwritten_hypothetical;Tell"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell 319 :: Tell;Simpson_ghostwritten_hypothetical"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N 320 :: O"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "terrible 321 :: horrible;unfortunate;unbelievable;unforgivable;miserable;horrible_horrible;unspeakable;abominable;Horrible"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "terrible 322 :: unforgivable;horrible;abominable;miserable;unfortunate;unbelievable;unspeakable;Horrible;horrible_horrible"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning: u.s. is not in entire corpus\n",
        "terrible 323 :: horrible;unforgivable;abominable;miserable;unbelievable;unfortunate;unspeakable;horrible_horrible;Horrible"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "terrible 324 :: horrible;horrible_horrible;unforgivable;abominable;Horrible;miserable;unspeakable;unbelievable;unfortunate"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "terrible 325 :: horrible;Horrible;horrible_horrible;unforgivable;unfortunate;abominable;unbelievable;unspeakable;miserable"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "terrible 326 :: horrible;abominable;unfortunate;Horrible;miserable;unbelievable;unforgivable;unspeakable;horrible_horrible"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning: reality. is not in entire corpus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-33-585a32b54aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m#print sentence, word, index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_replacements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" :: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-32-a9bf4b7eabb8>\u001b[0m in \u001b[0;36mfind_replacements\u001b[0;34m(sentence, lwindow, rwindow, add)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_replacements_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrwindow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"NONE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-32-a9bf4b7eabb8>\u001b[0m in \u001b[0;36mfind_replacements_helper\u001b[0;34m(imp_words, word, index, lwindow, rwindow, add)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Ignore itself as a replacement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreplacement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/bansal/anaconda/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}